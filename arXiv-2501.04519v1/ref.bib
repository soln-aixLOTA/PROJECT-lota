@inproceedings{top,
	author = {Li, Junyan and Zhang, Li Lyna and Xu, Jiahang and Wang, Yujing and Yan, Shaoguang and Xia, Yunqing and Yang, Yuqing and Cao, Ting and Sun, Hao and Deng, Weiwei and Zhang, Qi and Yang, Mao},
	title = {Constraint-Aware and Ranking-Distilled Token Pruning for Efficient Transformer Inference},
	year = {2023},
	booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {1280–1290},
	series = {KDD '23}
}
@inproceedings{ltp,
	author = {Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
	title = {Learned Token Pruning for Transformers},
	year = {2022},
	isbn = {9781450393850},
	publisher = {Association for Computing Machinery},
	booktitle = {Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {784–794},
	numpages = {11},
	keywords = {deep learning, natural language processing, network pruning},
	location = {Washington DC, USA},
	series = {KDD '22}
}

@inproceedings{transkimmer,
	title = {Transkimmer: Transformer Learns to Layer-wise Skim},
	author = {Guan, Yue  and
	Li, Zhengyi  and
	Leng, Jingwen  and
	Lin, Zhouhan  and
	Guo, Minyi},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	year = {2022},
	publisher = {Association for Computational Linguistics},
	pages = {7275--7286},
	
}
@inproceedings{swiftpruner,
	title={SwiftPruner: Reinforced Evolutionary Pruning for Efficient Ad Relevance},
	author={Zhang, Li Lyna and Homma, Youkow and Wang, Yujing and Wu, Min and Yang, Mao and Zhang, Ruofei and Cao, Ting and Shen, Wei},
	booktitle={Proceedings of the 31st ACM International Conference on Information \& Knowledge Management},
	pages={3654--3663},
	year={2022}
}
@inproceedings{cofi,
	title={Structured Pruning Learns Compact and Accurate Models},
	author={Xia, Mengzhou and Zhong, Zexuan and Chen, Danqi},
	booktitle={Association for Computational Linguistics (ACL)},
	year={2022}
}
@inproceedings{sanh2020distilbert,
	title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter}, 
	author={Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
	year={2020},
	eprint={1910.01108},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{jiao2020tinybert,
	title={TinyBERT: Distilling BERT for Natural Language Understanding}, 
	author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
	year={2020},
	booktitle={EMNLP}
}

@article{zafrir2019q8bert,
	title={Q8bert: Quantized 8bit bert},
	author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
	journal={arXiv preprint arXiv:1910.06188},
	year={2019}
}
@inproceedings{movement,
	title={Movement pruning: Adaptive sparsity by fine-tuning},
	author={Sanh, Victor and Wolf, Thomas and Rush, Alexander M},
	booktitle={NeurIPS},
	year={2020}
}
@inproceedings{gordon2020compressing,
	title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning}, 
	author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
	year={2020},
	eprint={2002.08307},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{shen2020q,
	title={Q-bert: Hessian based ultra low precision quantization of bert},
	author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	year={2020}
}
@article{kim2021bert,
	title={I-bert: Integer-only bert quantization},
	author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
	journal={arXiv preprint arXiv:2101.01321},
	year={2021}
}
@article{bert,
	title={Bert: Pre-training of deep bidirectional transformers for language understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	journal={arXiv preprint arXiv:1810.04805},
	year={2018}
}
@article{albert,
	title={Albert: A lite bert for self-supervised learning of language representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	journal={arXiv preprint arXiv:1909.11942},
	year={2019}
}
@inproceedings{ye2021tr,
  title={TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference},
  author={Ye, Deming and Lin, Yankai and Huang, Yufei and Sun, Maosong},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5798--5809},
  year={2021}
}
@inproceedings{nn_pruning,
	author = {Francois Lagunas and Ella Charlaix and Victor Sanh and Alexander M. Rush},
	title = {Block Pruning For Faster Transformers},
	year = {2021},
	
	booktitle = {EMNLP},
	
}
@article{roberta,
	title={Roberta: A robustly optimized bert pretraining approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}
@article{t5,
	author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
	title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	journal = {Journal of Machine Learning Research},
	year    = {2020},
	volume  = {21},
	number  = {140},
	pages   = {1-67},
	url     = {http://jmlr.org/papers/v21/20-074.html}
}
@inproceedings{llmpruner,
	title={LLM-Pruner: On the Structural Pruning of Large Language Models},
	author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
	journal={arXiv preprint arXiv:2305.11627},
	year={2023}
}
@article{loraprune,
	title={Pruning Meets Low-Rank Parameter-Efficient Fine-Tuning},
	author={Zhang, Mingyang and Shen, Chunhua and Yang, Zhen and Ou, Linlin and Yu, Xinyi and Zhuang, Bohan and others},
	journal={arXiv preprint arXiv:2305.18403},
	year={2023}
}
@article{sparsegpt,
	title={{SparseGPT}: Massive Language Models Can Be Accurately Pruned in One-Shot}, 
	author={Elias Frantar and Dan Alistarh},
	year={2023},
	journal={arXiv preprint arXiv:2301.00774}
}

@article{wanda,
	title={A Simple and Effective Pruning Approach for Large Language Models},
	author={Sun, Mingjie and Liu, Zhuang and Bair, Anna and Kolter, J Zico},
	journal={arXiv preprint arXiv:2306.11695},
	year={2023}
}

@inproceedings{lora,
	title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
	author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
	booktitle={International Conference on Learning Representations},
	year={2022},
	url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@article{llama,
	title={Llama: Open and efficient foundation language models},
	author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
	journal={arXiv preprint arXiv:2302.13971},
	year={2023}
}
@article{llama2,
	title={Llama 2: Open foundation and fine-tuned chat models},
	author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
	journal={arXiv preprint arXiv:2307.09288},
	year={2023}
}
@article{transformer,
	title={Attention is all you need},
	author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	journal={Advances in neural information processing systems},
	volume={30},
	year={2017}
}

@article{ste,
	title={Estimating or propagating gradients through stochastic neurons for conditional computation},
	author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
	journal={arXiv preprint arXiv:1308.3432},
	year={2013}
}
@article{wei2021finetuned,
	title={Finetuned language models are zero-shot learners},
	author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
	journal={arXiv preprint arXiv:2109.01652},
	year={2021}
}
@inproceedings{
	sanh2022multitask,
	title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
	author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
	booktitle={International Conference on Learning Representations},
	year={2022},
}
@article{peng2023instruction,
	title={Instruction tuning with gpt-4},
	author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
	journal={arXiv preprint arXiv:2304.03277},
	year={2023}
}
@article{instructgpt,
	title={Training language models to follow instructions with human feedback},
	author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={27730--27744},
	year={2022}
}
@article{compressprompt,
	title={Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt},
	author={Xu, Zhaozhuo and Liu, Zirui and Chen, Beidi and Tang, Yuxin and Wang, Jue and Zhou, Kaixiong and Hu, Xia and Shrivastava, Anshumali},
	journal={arXiv preprint arXiv:2305.11186},
	year={2023}
}
@article{goyal2022news,
	title={News summarization and evaluation in the era of gpt-3},
	author={Goyal, Tanya and Li, Junyi Jessy and Durrett, Greg},
	journal={arXiv preprint arXiv:2209.12356},
	year={2022}
}
@article{cot,
	title={Chain-of-thought prompting elicits reasoning in large language models},
	author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={24824--24837},
	year={2022}
}
@inproceedings{schick-schutze-2021-exploiting,
	title = {Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference},
	author = {Schick, Timo  and
	Sch{\"u}tze, Hinrich},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	year = {2021},
}
@article{chowdhery2022palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}
@article{chung2022scaling,
	title={Scaling instruction-finetuned language models},
	author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
	journal={arXiv preprint arXiv:2210.11416},
	year={2022}
}
@inproceedings{alpaca,
	author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
	title = {Stanford Alpaca: An Instruction-following LLaMA model},
	year = {2023},
	publisher = {GitHub},
	journal = {GitHub repository},
	howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{selfinstruct,
	title={Self-instruct: Aligning language model with self generated instructions},
	author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
	journal={arXiv preprint arXiv:2212.10560},
	year={2022}
}
@inproceedings{vicuna,
	title = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
	url = {https://lmsys.org/blog/2023-03-30-vicuna/},
	author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
	month = {March},
	year = {2023}
}
@article{liu2023llm,
	title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
	author={Liu, Zechun and Oguz, Barlas and Zhao, Changsheng and Chang, Ernie and Stock, Pierre and Mehdad, Yashar and Shi, Yangyang and Krishnamoorthi, Raghuraman and Chandra, Vikas},
	journal={arXiv preprint arXiv:2305.17888},
	year={2023}
}
@article{frantar-gptq,
	title={{GPTQ}: Accurate Post-training Compression for Generative Pretrained Transformers}, 
	author={Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
	year={2022},
	journal={arXiv preprint arXiv:2210.17323}
}
@article{yao2022zeroquant,
	title={Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
	author={Yao, Zhewei and Yazdani Aminabadi, Reza and Zhang, Minjia and Wu, Xiaoxia and Li, Conglong and He, Yuxiong},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={27168--27183},
	year={2022}
}
@inproceedings{xiao2023smoothquant,
	title={Smoothquant: Accurate and efficient post-training quantization for large language models},
	author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
	booktitle={International Conference on Machine Learning},
	pages={38087--38099},
	year={2023},
	organization={PMLR}
}
@article{zhao2023survey,
	title={A survey of large language models},
	author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
	journal={arXiv preprint arXiv:2303.18223},
	year={2023}
}
@article{huang2022towards,
	title={Towards reasoning in large language models: A survey},
	author={Huang, Jie and Chang, Kevin Chen-Chuan},
	journal={arXiv preprint arXiv:2212.10403},
	year={2022}
}
@article{chang2023survey,
	title={A survey on evaluation of large language models},
	author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Zhu, Kaijie and Chen, Hao and Yang, Linyi and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and others},
	journal={arXiv preprint arXiv:2307.03109},
	year={2023}
}
@inproceedings{gpt3,
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Language Models are Few-Shot Learners},
	volume = {33},
	year = {2020}
}
@article{opt,
	title={Opt: Open pre-trained transformer language models},
	author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
	journal={arXiv preprint arXiv:2205.01068},
	year={2022}
}
@article{palm,
	title={Palm: Scaling language modeling with pathways},
	author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
	journal={arXiv preprint arXiv:2204.02311},
	year={2022}
}
@article{bloom,
	title={Bloom: A 176b-parameter open-access multilingual language model},
	author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
	journal={arXiv preprint arXiv:2211.05100},
	year={2022}
}
@article{rlprompt,
	title={Rlprompt: Optimizing discrete text prompts with reinforcement learning},
	author={Deng, Mingkai and Wang, Jianyu and Hsieh, Cheng-Ping and Wang, Yihan and Guo, Han and Shu, Tianmin and Song, Meng and Xing, Eric P and Hu, Zhiting},
	journal={arXiv preprint arXiv:2205.12548},
	year={2022}
}
@article{ge2023context,
	title={In-context Autoencoder for Context Compression in a Large Language Model},
	author={Ge, Tao and Hu, Jing and Wang, Xun and Chen, Si-Qing and Wei, Furu},
	journal={arXiv preprint arXiv:2307.06945},
	year={2023}
}
@article{mu2023learning,
	title={Learning to compress prompts with gist tokens},
	author={Mu, Jesse and Li, Xiang Lisa and Goodman, Noah},
	journal={arXiv preprint arXiv:2304.08467},
	year={2023}
}

@article{chevalier2023adapting,
	title={Adapting Language Models to Compress Contexts},
	author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
	journal={arXiv preprint arXiv:2305.14788},
	year={2023}
}

@article{wingate2022prompt,
	title={Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models},
	author={Wingate, David and Shoeybi, Mohammad and Sorensen, Taylor},
	journal={arXiv preprint arXiv:2210.03162},
	year={2022}
}
@article{peng2023yarn,
	title={Yarn: Efficient context window extension of large language models},
	author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	journal={arXiv preprint arXiv:2309.00071},
	year={2023}
}
@article{tworkowski2023focused,
	title={Focused Transformer: Contrastive Training for Context Scaling}, 
	author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
	year={2023},
	eprint={2307.03170},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{llmlingua,
	title = "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
	author = "Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
	booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing",
	month = dec,
	year = "2023",

}
@article{jiang-etal-2023-longllmlingua,
	title = "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression",
	author = "Huiqiang Jiang and Qianhui Wu and and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu",
	journal = "ArXiv preprint",
	volume = "abs/2310.06839",
	year = "2023",
}

@inproceedings{gpt4,
	title={GPT-4 Technical Report}, 
	author={OpenAI},
	year={2023},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{chatgptdoc,
	title={Welcome to the OpenAI platform}, 
	author={OpenAI},
	year={2023},
	url={
	https://platform.openai.com/docs/introduction}
}
@article{liu2023lost,
	title={Lost in the middle: How language models use long contexts},
	author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	journal={arXiv preprint arXiv:2307.03172},
	year={2023}
}
@article{chen2023takes,
	title={It Takes One to Tango but More Make Trouble? In-Context Training with Different Number of Demonstrations},
	author={Chen, Jiuhai and Chen, Lichang and Zhou, Tianyi},
	journal={arXiv preprint arXiv:2303.08119},
	year={2023}
}
@inproceedings{
	selfconsistency,
	title={Self-Consistency Improves Chain of Thought Reasoning in Language Models},
	author={Xuezhi Wang and Jason Wei and Dale Schuurmans and Quoc V Le and Ed H. Chi and Sharan Narang and Aakanksha Chowdhery and Denny Zhou},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023},
	url={https://openreview.net/forum?id=1PL1NIMMrw}
}

@article{pot,
	title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
	author = {Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
	journal={Transactions on Machine Learning Research},
	year = {2023},
}

@article{topk,
	title={What Makes Good In-Context Examples for GPT-$3 $?},
	author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
	journal={arXiv preprint arXiv:2101.06804},
	year={2021}
}
@article{rubin2021learning,
	title={Learning to retrieve prompts for in-context learning},
	author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
	journal={arXiv preprint arXiv:2112.08633},
	year={2021}
}
@article{lu2021fantastically,
	title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
	author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
	journal={arXiv preprint arXiv:2104.08786},
	year={2021}
}
@article{ye2023compositional,
	title={Compositional exemplars for in-context learning},
	author={Ye, Jiacheng and Wu, Zhiyong and Feng, Jiangtao and Yu, Tao and Kong, Lingpeng},
	journal={arXiv preprint arXiv:2302.05698},
	year={2023}
}
@article{scarlatos2023reticl,
	title={RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning},
	author={Scarlatos, Alexander and Lan, Andrew},
	journal={arXiv preprint arXiv:2305.14502},
	year={2023}
}
@article{wu2022self,
	title={Self-adaptive in-context learning},
	author={Wu, Zhiyong and Wang, Yaoxiang and Ye, Jiacheng and Kong, Lingpeng},
	journal={arXiv preprint arXiv:2212.10375},
	year={2022}
}
@article{fu2022complexity,
	title={Complexity-based prompting for multi-step reasoning},
	author={Fu, Yao and Peng, Hao and Sabharwal, Ashish and Clark, Peter and Khot, Tushar},
	journal={arXiv preprint arXiv:2210.00720},
	year={2022}
}

@article{xu2023wizardlm,
	title={Wizardlm: Empowering large language models to follow complex instructions},
	author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
	journal={arXiv preprint arXiv:2304.12244},
	year={2023}
}
@article{reinforce,
	author = {Williams, Ronald J.},
	title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
	year = {1992},
	booktitle={Reinforcemend Learning}
	
}
@inproceedings{gao2021making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  booktitle={Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL-IJCNLP 2021},
  pages={3816--3830},
  year={2021},
  organization={Association for Computational Linguistics (ACL)}
}
@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}
@inproceedings{hongjin2022selective,
  title={Selective Annotation Makes Language Models Better Few-Shot Learners},
  author={Hongjin, SU and Kasai, Jungo and Wu, Chen Henry and Shi, Weijia and Wang, Tianlu and Xin, Jiayi and Zhang, Rui and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@inproceedings{lu2022fantastically,
  title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8086--8098},
  year={2022}
}
@article{wang2023learning,
  title={Learning to Retrieve In-Context Examples for Large Language Models},
  author={Wang, Liang and Yang, Nan and Wei, Furu},
  journal={arXiv preprint arXiv:2307.07164},
  year={2023}
}
@article{qin2023context,
  title={In-Context Learning with Iterative Demonstration Selection},
  author={Qin, Chengwei and Zhang, Aston and Dagar, Anirudh and Ye, Wenming},
  journal={arXiv preprint arXiv:2310.09881},
  year={2023}
}
@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}
@article{yue2023mammoth,
  title={Mammoth: Building math generalist models through hybrid instruction tuning},
  author={Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}
@article{yu2023metamath,
  title={Metamath: Bootstrap your own mathematical questions for large language models},
  author={Yu, Longhui and Jiang, Weisen and Shi, Han and Yu, Jincheng and Liu, Zhengying and Zhang, Yu and Kwok, James T and Li, Zhenguo and Weller, Adrian and Liu, Weiyang},
  journal={arXiv preprint arXiv:2309.12284},
  year={2023}
}
@inproceedings{zhou2022least,
  title={Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc V and others},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}
@inproceedings{hendrycks2021measuring,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)},
  year={2021}
}
@article{longllama,
	title={Focused Transformer: Contrastive Training for Context Scaling}, 
	author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
	year={2023},
	eprint={2307.03170},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@article{wang2023augmenting,
	title={Augmenting Language Models with Long-Term Memory},
	author={Wang, Weizhi and Dong, Li and Cheng, Hao and Liu, Xiaodong and Yan, Xifeng and Gao, Jianfeng and Wei, Furu},
	journal={arXiv preprint arXiv:2306.07174},
	year={2023}
}
@article{
	pot,
	title={Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},
	author={Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},
	journal={Transactions on Machine Learning Research},
	issn={2835-8856},
	year={2023},
	url={https://openreview.net/forum?id=YfZ4ZPt8zd},
	note={}
}

@article{tot,
	title={Tree of thoughts: Deliberate problem solving with large language models},
	author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{han2023lminfinite,
	title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models},
	author={Han, Chi and Wang, Qifan and Xiong, Wenhan and Chen, Yu and Ji, Heng and Wang, Sinong},
	journal={arXiv preprint arXiv:2308.16137},
	year={2023}
}
@article{du2023improving,
	title={Improving factuality and reasoning in language models through multiagent debate},
	author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
	journal={arXiv preprint arXiv:2305.14325},
	year={2023}
}
@article{streamingllm,
	title={Efficient Streaming Language Models with Attention Sinks},
	author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
	journal={arXiv},
	year={2023}
}
@article{dao2023flashattention2,
	title={Flash{A}ttention-2: Faster Attention with Better Parallelism and Work Partitioning},
	author={Dao, Tri},
	year={2023}
}
@inproceedings{ntk,
	title={Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degration},
	author={LocalLLaMA},
	year={2023},
	url={\url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}}
}

@inproceedings{dynamicntk,
	title={Dynamically scaled rope further increases performance of long context LLaMA with zero fine-tuning},
	author={LocalLLaMA},
	year={2023},
	url={\url{https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}}
}
@article{yarn,
	title={Yarn: Efficient context window extension of large language models},
	author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
	journal={arXiv preprint arXiv:2309.00071},
	year={2023}
}
@article{pi,
	title={Extending context window of large language models via positional interpolation},
	author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
	journal={arXiv preprint arXiv:2306.15595},
	year={2023}
}

@inproceedings{li2023compressing,
	title={Compressing Context to Enhance Inference Efficiency of Large Language Models}, 
	author={Yucheng Li and Bo Dong and Chenghua Lin and Frank Guerin},
	year={2023},
	eprint={2310.06201},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}
@inproceedings{ling2017program,
  title={Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems},
  author={Ling, Wang and Yogatama, Dani and Dyer, Chris and Blunsom, Phil},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={158--167},
  year={2017}
}

@article{kojima2022large,
	title={Large language models are zero-shot reasoners},
	author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={22199--22213},
	year={2022}
}
@article{wu2023openicl,
  title={OpenICL: An Open-Source Framework for In-context Learning},
  author={Zhenyu, Wu and Yaoxiang, Wang and Jiacheng, Ye and Jiangtao, Feng and Jingjing, Xu and Yu, Qiao and Zhiyong, Wu},
  journal={arXiv preprint arXiv:2303.02913},
  year={2023}
}


@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@article{lewkowycz2022solving,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{rap,
	title={Reasoning with language model is planning with world model},
	author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
	journal={arXiv preprint arXiv:2305.14992},
	year={2023}
}

@article{hao2022structured,
	title={Structured prompting: Scaling in-context learning to 1,000 examples},
	author={Hao, Yaru and Sun, Yutao and Dong, Li and Han, Zhixiong and Gu, Yuxian and Wei, Furu},
	journal={arXiv preprint arXiv:2212.06713},
	year={2022}
}
@article{ding2023everything,
	title={Everything of Thoughts: Defying the Law of Penrose Triangle for Thought Generation},
	author={Ding, Ruomeng and Zhang, Chaoyun and Wang, Lu and Xu, Yong and Ma, Minghua and Zhang, Wei and Qin, Si and Rajmohan, Saravan and Lin, Qingwei and Zhang, Dongmei},
	journal={arXiv preprint arXiv:2311.04254},
	year={2023}
}
@inproceedings{shin2020autoprompt,
  title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={4222--4235},
  year={2020}
}

@article{tora,
title={Tora: A tool-integrated reasoning agent for mathematical problem solving},
author={Gou, Zhibin and Shao, Zhihong and Gong, Yeyun and Yang, Yujiu and Huang, Minlie and Duan, Nan and Chen, Weizhu and others},
journal={arXiv preprint arXiv:2309.17452},
year={2023}
}

@inproceedings{mawps,
	title = "{MAWPS}: A Math Word Problem Repository",
	author = "Koncel-Kedziorski, Rik  and
	Roy, Subhro  and
	Amini, Aida  and
	Kushman, Nate  and
	Hajishirzi, Hannaneh",
	editor = "Knight, Kevin  and
	Nenkova, Ani  and
	Rambow, Owen",
	booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
	year = "2016",
	pages = "1152--1157",
}
@inproceedings{svamp,
  title={Are NLP Models really able to Solve Simple Math Word Problems?},
  author={Patel, Arkil and Bhattamishra, Satwik and Goyal, Navin},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2080--2094},
  year={2021}
}
@inproceedings{multiarith,
  title={Solving General Arithmetic Word Problems},
  author={Roy, Subhro and Roth, Dan},
  booktitle={Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages={1743--1752},
  year={2015}
}
@inproceedings{addsub,
  title={Learning to solve arithmetic word problems with verb categorization},
  author={Hosseini, Mohammad Javad and Hajishirzi, Hannaneh and Etzioni, Oren and Kushman, Nate},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={523--533},
  year={2014}
}
@article{singleeq,
  title={Parsing algebraic word problems into equations},
  author={Koncel-Kedziorski, Rik and Hajishirzi, Hannaneh and Sabharwal, Ashish and Etzioni, Oren and Ang, Siena Dumas},
  journal={Transactions of the Association for Computational Linguistics},
  volume={3},
  pages={585--597},
  year={2015},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}
@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}
@inproceedings{dai2023can,
  title={Why can GPT learn in-context? language models secretly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}
@article{wang2023label,
  title={Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning},
  author={Wang, Lean and Li, Lei and Dai, Damai and Chen, Deli and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
  journal={arXiv preprint arXiv:2305.14160},
  year={2023}
}
@inproceedings{yoo2022ground,
  title={Ground-Truth Labels Matter: A Deeper Look into Input-Label Demonstrations},
  author={Yoo, Kang Min and Kim, Junyeob and Kim, Hyuhng Joon and Cho, Hyunsoo and Jo, Hwiyeol and Lee, Sang-Woo and Lee, Sang-goo and Kim, Taeuk},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={2422--2437},
  year={2022}
}
@inproceedings{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  booktitle={Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
  pages={11048--11064},
  year={2022}
}
@article{fu2023chain,
  title={Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models' Reasoning Performance},
  author={Fu, Yao and Ou, Litu and Chen, Mingyu and Wan, Yuhao and Peng, Hao and Khot, Tushar},
  journal={arXiv preprint arXiv:2305.17306},
  year={2023}
}

@inproceedings{weng2023large,
	title={Large Language Models are Better Reasoners with Self-Verification}, 
	author={Yixuan Weng and Minjun Zhu and Fei Xia and Bin Li and Shizhu He and Shengping Liu and Bin Sun and Kang Liu and Jun Zhao},
	year={2023},
	eprint={2212.09561},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
}

@misc{gemini,
	title={Gemini: A Family of Highly Capable Multimodal Models}, 
	author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
	year={2024},
	eprint={2312.11805},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@inproceedings{
	valmeekam2022large,
	title={Large Language Models Still Can't Plan (A Benchmark for {LLM}s on Planning and Reasoning about Change)},
	author={Karthik Valmeekam and Alberto Olmo and Sarath Sreedharan and Subbarao Kambhampati},
	booktitle={NeurIPS 2022 Foundation Models for Decision Making Workshop},
	year={2022},
	url={https://openreview.net/forum?id=wUU-7XTL5XO}
}


@inproceedings{
	wang2024mathcoder,
	title={MathCoder: Seamless Code Integration in {LLM}s for Enhanced Mathematical Reasoning},
	author={Ke Wang and Houxing Ren and Aojun Zhou and Zimu Lu and Sichun Luo and Weikang Shi and Renrui Zhang and Linqi Song and Mingjie Zhan and Hongsheng Li},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	url={https://openreview.net/forum?id=z8TW0ttBPp}
}

article{yuan2023scaling,
	title={Scaling relationship on learning mathematical reasoning with large language models},
	author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Tan, Chuanqi and Zhou, Chang},
	journal={arXiv preprint arXiv:2308.01825},
	year={2023}
}

@article{xi2024training,
	title={Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning},
	author={Xi, Zhiheng and Chen, Wenxiang and Hong, Boyang and Jin, Senjie and Zheng, Rui and He, Wei and Ding, Yiwen and Liu, Shichun and Guo, Xin and Wang, Junzhe and others},
	journal={arXiv preprint arXiv:2402.05808},
	year={2024}
}

@article{shwartz2020unsupervised,
	title={Unsupervised commonsense question answering with self-talk},
	author={Shwartz, Vered and West, Peter and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	journal={arXiv preprint arXiv:2004.05483},
	year={2020}
}

@article{nye2021show,
	title={Show your work: Scratchpads for intermediate computation with language models},
	author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
	journal={arXiv preprint arXiv:2112.00114},
	year={2021}
}

@article{wang2024chain,
	title={Chain-of-Thought Reasoning Without Prompting},
	author={Wang, Xuezhi and Zhou, Denny},
	journal={arXiv preprint arXiv:2402.10200},
	year={2024}
}

@article{li2023textbooks,
	title={Textbooks are all you need ii: phi-1.5 technical report},
	author={Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
	journal={arXiv preprint arXiv:2309.05463},
	year={2023}
}

@article{zelikman2022star,
	title={Star: Bootstrapping reasoning with reasoning},
	author={Zelikman, Eric and Wu, Yuhuai and Mu, Jesse and Goodman, Noah},
	journal={Advances in Neural Information Processing Systems},
	volume={35},
	pages={15476--15488},
	year={2022}
}

@article{hosseini2024v,
	title={V-star: Training verifiers for self-taught reasoners},
	author={Hosseini, Arian and Yuan, Xingdi and Malkin, Nikolay and Courville, Aaron and Sordoni, Alessandro and Agarwal, Rishabh},
	journal={arXiv preprint arXiv:2402.06457},
	year={2024}
}

@inproceedings{phan2023training,
	title={Training chain-of-thought via latent-variable inference},
	author={Phan, Du and Hoffman, Matthew Douglas and Douglas, Sholto and Le, Tuan Anh and Parisi, Aaron T and Sountsov, Pavel and Sutton, Charles and Vikram, Sharad and Saurous, Rif A and others},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023}
}

@article{zelikman2024quiet,
	title={Quiet-star: Language models can teach themselves to think before speaking},
	author={Zelikman, Eric and Harik, Georges and Shao, Yijia and Jayasiri, Varuna and Haber, Nick and Goodman, Noah D},
	journal={arXiv preprint arXiv:2403.09629},
	year={2024}
}

@inproceedings{besta2024graph,
	title={Graph of thoughts: Solving elaborate problems with large language models},
	author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and others},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={16},
	pages={17682--17690},
	year={2024}
}

@article{chen2022program,
	title={Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
	author={Chen, Wenhu and Ma, Xueguang and Wang, Xinyi and Cohen, William W},
	journal={arXiv preprint arXiv:2211.12588},
	year={2022}
}

@article{zhou2023solving,
	title={Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification},
	author={Zhou, Aojun and Wang, Ke and Lu, Zimu and Shi, Weikang and Luo, Sichun and Qin, Zipeng and Lu, Shaoqing and Jia, Anya and Song, Linqi and Zhan, Mingjie and others},
	journal={arXiv preprint arXiv:2308.07921},
	year={2023}
}

@article{madaan2024self,
	title={Self-refine: Iterative refinement with self-feedback},
	author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}

@article{chen2024boosting,
	title={Boosting of thoughts: Trial-and-error problem solving with large language models},
	author={Chen, Sijia and Li, Baochun and Niu, Di},
	journal={arXiv preprint arXiv:2402.11140},
	year={2024}
}

@article{zhou2024self,
	title={Self-discover: Large language models self-compose reasoning structures},
	author={Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
	journal={arXiv preprint arXiv:2402.03620},
	year={2024}
}

@article{khot2022decomposed,
	title={Decomposed prompting: A modular approach for solving complex tasks},
	author={Khot, Tushar and Trivedi, Harsh and Finlayson, Matthew and Fu, Yao and Richardson, Kyle and Clark, Peter and Sabharwal, Ashish},
	journal={arXiv preprint arXiv:2210.02406},
	year={2022}
}

@article{zheng2023take,
	title={Take a step back: Evoking reasoning via abstraction in large language models},
	author={Zheng, Huaixiu Steven and Mishra, Swaroop and Chen, Xinyun and Cheng, Heng-Tze and Chi, Ed H and Le, Quoc V and Zhou, Denny},
	journal={arXiv preprint arXiv:2310.06117},
	year={2023}
}

@article{han2022folio,
	title={Folio: Natural language reasoning with first-order logic},
	author={Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Benson, Luke and Sun, Lucy and Zubova, Ekaterina and Qiao, Yujie and Burtell, Matthew and others},
	journal={arXiv preprint arXiv:2209.00840},
	year={2022}
}

@article{MATH,
	title={Measuring mathematical problem solving with the math dataset},
	author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2103.03874},
	year={2021}
}

@article{patel2022question,
	title={Is a question decomposition unit all we need?},
	author={Patel, Pruthvi and Mishra, Swaroop and Parmar, Mihir and Baral, Chitta},
	journal={arXiv preprint arXiv:2205.12538},
	year={2022}
}

@article{du2023improving,
	title={Improving factuality and reasoning in language models through multiagent debate},
	author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B and Mordatch, Igor},
	journal={arXiv preprint arXiv:2305.14325},
	year={2023}
}

@article{yang2018hotpotqa,
	title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
	author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
	journal={arXiv preprint arXiv:1809.09600},
	year={2018}
}

@article{shinn2023reflexion,
	title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
	author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
	journal={arXiv preprint arXiv:2303.11366},
	year={2023}
}

@article{ye2022unreliability,
	title={The unreliability of explanations in few-shot prompting for textual reasoning},
	author={Ye, Xi and Durrett, Greg},
	journal={Advances in neural information processing systems},
	volume={35},
	pages={30378--30392},
	year={2022}
}

@inproceedings{shi2023large,
	title={Large language models can be easily distracted by irrelevant context},
	author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
	booktitle={International Conference on Machine Learning},
	pages={31210--31227},
	year={2023},
	organization={PMLR}
}
@article{gero2023self,
title={Self-verification improves few-shot clinical information extraction},
author={Gero, Zelalem and Singh, Chandan and Cheng, Hao and Naumann, Tristan and Galley, Michel and Gao, Jianfeng and Poon, Hoifung},
journal={arXiv preprint arXiv:2306.00024},
year={2023}
}
@article{chen2024self,
	title={Self-play fine-tuning converts weak language models to strong language models},
	author={Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan},
	journal={arXiv preprint arXiv:2401.01335},
	year={2024}
}
@inproceedings{bi2024program,
	title={When Do Program-of-Thought Works for Reasoning?},
	author={Bi, Zhen and Zhang, Ningyu and Jiang, Yinuo and Deng, Shumin and Zheng, Guozhou and Chen, Huajun},
	booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
	volume={38},
	number={16},
	pages={17691--17699},
	year={2024}
}

@article{selfrefine,
	title={Self-refine: Iterative refinement with self-feedback},
	author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{selfevaluation,
	title={Self-evaluation improves selective generation in large language models},
	author={Ren, Jie and Zhao, Yao and Vu, Tu and Liu, Peter J and Lakshminarayanan, Balaji},
	journal={arXiv preprint arXiv:2312.09300},
	year={2023}
}
@article{selfrefinereport,
	title={Analyzing the performance of self-refine on different large language models},
	author={Anton Forsman},
	url={https://github.com/anforsm/self-refine/blob/main/report.pdf},
	year={2024}
}
@misc{wang2024qimprovingmultistepreasoning,
	title={Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning}, 
	author={Chaojie Wang and Yanchen Deng and Zhiyi Lv and Shuicheng Yan and An Bo},
	year={2024},
	eprint={2406.14283},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
	url={https://arxiv.org/abs/2406.14283}, 
}
@article{zhang2024accessing,
	title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B},
	author={Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
	journal={arXiv preprint arXiv:2406.07394},
	year={2024}
}
@misc{alphamath,
	title={AlphaMath Almost Zero: process Supervision without process}, 
	author={Guoxin Chen and Minpeng Liao and Chengxi Li and Kai Fan},
	year={2024},
	eprint={2405.03553},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
	url={https://arxiv.org/abs/2405.03553}, 
}
@misc{mathshepherd,
	title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
	author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
	year={2024},
	eprint={2312.08935},
	archivePrefix={arXiv},
	primaryClass={cs.AI}
	url={https://arxiv.org/abs/2312.08935}, 
}
@article{mitra2024orca,
	title={Orca-math: Unlocking the potential of slms in grade school math},
	author={Mitra, Arindam and Khanpour, Hamed and Rosset, Corby and Awadallah, Ahmed},
	journal={arXiv preprint arXiv:2402.14830},
	year={2024}
}
@article{xwin,
	title={Common 7b language models already possess strong math capabilities},
	author={Li, Chen and Wang, Weiqi and Hu, Jingcheng and Wei, Yixuan and Zheng, Nanning and Hu, Han and Zhang, Zheng and Peng, Houwen},
	journal={arXiv preprint arXiv:2403.04706},
	year={2024}
}
@article{zhang2024accessing,
	title={Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B},
	author={Zhang, Di and Li, Jiatong and Huang, Xiaoshui and Zhou, Dongzhan and Li, Yuqiang and Ouyang, Wanli},
	journal={arXiv preprint arXiv:2406.07394},
	year={2024}
}

@article{jiang2023mistral,
	title={Mistral 7B},
	author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
	journal={arXiv preprint arXiv:2310.06825},
	year={2023}
}

@article{phi3,
	title={Phi-3 technical report: A highly capable language model locally on your phone},
	author={Abdin, Marah and Jacobs, Sam Ade and Awan, Ammar Ahmad and Aneja, Jyoti and Awadallah, Ahmed and Awadalla, Hany and Bach, Nguyen and Bahree, Amit and Bakhtiari, Arash and Behl, Harkirat and others},
	journal={arXiv preprint arXiv:2404.14219},
	year={2024}
}

@article{hao2023reasoning,
  title={Reasoning with language model is planning with world model},
  author={Hao, Shibo and Gu, Yi and Ma, Haodi and Hong, Joshua Jiahua and Wang, Zhen and Wang, Daisy Zhe and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.14992},
  year={2023}
}

@article{wang2024q,
  title={Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning},
  author={Wang, Chaojie and Deng, Yanchen and Lv, Zhiyi and Yan, Shuicheng and Bo, An},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}
@inproceedings{utc,
	author = {Kocsis, Levente and Szepesvári, Csaba},
	year = {2006},
	month = {09},
	pages = {282-293},
	title = {Bandit Based Monte-Carlo Planning},
	volume = {2006},
	isbn = {978-3-540-45375-8},
	journal = {Machine Learning: ECML},
	doi = {10.1007/11871842_29}
}
@article{huang2023large,
	title={Large language models cannot self-correct reasoning yet},
	author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
	journal={arXiv preprint arXiv:2310.01798},
	year={2023}
}
@article{feng2023alphazero,
	title={Alphazero-like tree-search can guide large language model decoding and training},
	author={Feng, Xidong and Wan, Ziyu and Wen, Muning and Wen, Ying and Zhang, Weinan and Wang, Jun},
	journal={arXiv preprint arXiv:2309.17179},
	year={2023}
}


@inproceedings{kocsis2006bandit,
  title={Bandit based monte-carlo planning},
  author={Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle={European conference on machine learning},
  pages={282--293},
  year={2006},
  organization={Springer}
}
@misc{llama3,
	title={Introducing Meta Llama3: The most capable openly available LLM to date}, 
	author={Meta},
	year={2024},
 url={https://ai.meta.com/blog/meta-llama-3/}
}

@article{kang2024mindstar,
	title={MindStar: Enhancing Math Reasoning in Pre-trained LLMs at Inference Time},
	author={Kang, Jikun and Li, Xin Zhe and Chen, Xi and Kazemi, Amirreza and Chen, Boxing},
	journal={arXiv preprint arXiv:2405.16265},
	year={2024}
}

@article{alphago,
	title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
	author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
	journal={arXiv preprint arXiv:1712.01815},
	year={2017}
}
@article{gsmhard,
	title={PAL: Program-aided Language Models},
	author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
	journal={arXiv preprint arXiv:2211.10435},
	year={2022}
}
@article{math,
	title={Measuring mathematical problem solving with the math dataset},
	author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
	journal={arXiv preprint arXiv:2103.03874},
	year={2021}
}

@article{strategya,
	title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
	author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
	journal={Transactions of the Association for Computational Linguistics},
	volume={9},
	pages={346--361},
	year={2021},
	url={https://huggingface.co/datasets/ChilleD/StrategyQA}
}



@article{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal={arXiv preprint arXiv:2305.20050},
  year={2023}
}
@article{he2024olympiadbench,
	title={Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
	author={He, Chaoqun and Luo, Renjie and Bai, Yuzhuo and Hu, Shengding and Thai, Zhen Leng and Shen, Junhao and Hu, Jinyi and Han, Xu and Huang, Yujie and Zhang, Yuxiang and others},
	journal={arXiv preprint arXiv:2402.14008},
	year={2024}
}

@article{liao2024mario,
	title={MARIO: MAth Reasoning with code Interpreter Output--A Reproducible Pipeline},
	author={Liao, Minpeng and Luo, Wei and Li, Chengxi and Wu, Jing and Fan, Kai},
	journal={arXiv preprint arXiv:2401.08190},
	year={2024}
}
@inproceedings{RoyRo15,
	author = {Subhro Roy and Dan Roth},
	title = {{Solving General Arithmetic Word Problems}},
	booktitle = {Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	year = {2015},
	url = "http://cogcomp.org/papers/arithmetic.pdf",
}
@article{wu2024large,
	title={Large Language Models Can Self-Correct with Minimal Effort},
	author={Wu, Zhenyu and Zeng, Qingkai and Zhang, Zhihan and Tan, Zhaoxuan and Shen, Chao and Jiang, Meng},
	journal={arXiv preprint arXiv:2405.14092},
	year={2024}
}
@article{zhou2024self,
	title={Self-discover: Large language models self-compose reasoning structures},
	author={Zhou, Pei and Pujara, Jay and Ren, Xiang and Chen, Xinyun and Cheng, Heng-Tze and Le, Quoc V and Chi, Ed H and Zhou, Denny and Mishra, Swaroop and Zheng, Huaixiu Steven},
	journal={arXiv preprint arXiv:2402.03620},
	year={2024}
}
@misc{snell2024scalingllmtesttimecompute,
	title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
	author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
	year={2024},
	eprint={2408.03314},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/2408.03314}, 
}
@article{brown2024large,
	title={Large Language Monkeys: Scaling Inference Compute with Repeated Sampling},
	author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
	journal={arXiv preprint arXiv:2407.21787},
	year={2024}
}


@article{luo2024improve,
	title={Improve Mathematical Reasoning in Language Models by Automated Process Supervision},
	author={Luo, Liangchen and Liu, Yinxiao and Liu, Rosanne and Phatale, Samrat and Lara, Harsh and Li, Yunxuan and Shu, Lei and Zhu, Yun and Meng, Lei and Sun, Jiao and others},
	journal={arXiv preprint arXiv:2406.06592},
	year={2024}
}

@article{wang2024multi,
	title={Multi-step problem solving through a verifier: An empirical analysis on model-induced process supervision},
	author={Wang, Zihan and Li, Yunxuan and Wu, Yuexin and Luo, Liangchen and Hou, Le and Yu, Hongkun and Shang, Jingbo},
	journal={arXiv preprint arXiv:2402.02658},
	year={2024}
}

@article{rstar,
	title={Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers},
	author={Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
	journal={arXiv preprint arXiv:2408.06195},
	year={2024}
}
@article{snell2024scaling,
	title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
	author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
	journal={arXiv preprint arXiv:2408.03314},
	year={2024}
}

@inproceedings{
	llmjudge,
	title={Judging {LLM}-as-a-Judge with {MT}-Bench and Chatbot Arena},
	author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
	year={2023},
}

@inproceedings{
	lightman2024lets,
	title={Let's Verify Step by Step},
	author={Hunter Lightman and Vineet Kosaraju and Yuri Burda and Harrison Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	url={https://openreview.net/forum?id=v8L0pN6EOi}
}

@misc{numina_math_datasets,
	author = {Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu},
	title = {NuminaMath},
	year = {2024},
	publisher = {Numina},
	journal = {GitHub repository},
	howpublished = {\url{[https://github.com/project-numina/aimo-progress-prize](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)}}
}

@article{openmath2,
	title   = {OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data},
	author  = {Shubham Toshniwal and Wei Du and Ivan Moshkov and  Branislav Kisacanin and Alexan Ayrapetyan and Igor Gitman},
	year    = {2024},
	journal = {arXiv preprint arXiv:2410.01560}
}

@article{openmath1,
	title={Openmathinstruct-1: A 1.8 million math instruction tuning dataset},
	author={Toshniwal, Shubham and Moshkov, Ivan and Narenthiran, Sean and Gitman, Daria and Jia, Fei and Gitman, Igor},
	journal={arXiv preprint arXiv:2402.10176},
	year={2024}
}

@article{mathscale,
	title={Mathscale: Scaling instruction tuning for mathematical reasoning},
	author={Tang, Zhengyang and Zhang, Xingxing and Wan, Benyou and Wei, Furu},
	journal={arXiv preprint arXiv:2403.02884},
	year={2024}
}

@article{huang2024key,
	title={Key-point-driven data synthesis with its enhancement on mathematical reasoning},
	author={Huang, Yiming and Liu, Xiao and Gong, Yeyun and Gou, Zhibin and Shen, Yelong and Duan, Nan and Chen, Weizhu},
	journal={arXiv preprint arXiv:2403.02333},
	year={2024}
}

@article{snell2024scaling,
	title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
	author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
	journal={arXiv preprint arXiv:2408.03314},
	year={2024}
}

@article{wu2024empirical,
	title={An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models},
	author={Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
	journal={arXiv preprint arXiv:2408.00724},
	year={2024}
}

@article{brown2024large,
	title={Large language monkeys: Scaling inference compute with repeated sampling},
	author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
	journal={arXiv preprint arXiv:2407.21787},
	year={2024}
}

@article{o1,
	title   = {Openai o1 system card},
	author  = {OpenAI},
	year    = {2024},
	journal = {preprint}
}

@article{thinking,
	title={Thinking, fast and slow},
	author={Kahneman Daniel},
	year={2011},
	journal={Macmillan}

}

@article{stechly2024chain,
	title={Chain of thoughtlessness: An analysis of cot in planning},
	author={Stechly, Kaya and Valmeekam, Karthik and Kambhampati, Subbarao},
	journal={arXiv preprint arXiv:2405.04776},
	year={2024}
}

@article{valmeekam2023planning,
	title={On the planning abilities of large language models (a critical investigation with a proposed benchmark)},
	author={Valmeekam, Karthik and Sreedharan, Sarath and Marquez, Matthew and Olmo, Alberto and Kambhampati, Subbarao},
	journal={arXiv preprint arXiv:2302.06706},
	year={2023}
}
@article{wu2024comparative,
	title={A Comparative Study on Reasoning Patterns of OpenAI's o1 Model},
	author={Wu, Siwei and Peng, Zhongyuan and Du, Xinrun and Zheng, Tuney and Liu, Minghao and Wu, Jialong and Ma, Jiachen and Li, Yizhi and Yang, Jian and Zhou, Wangchunshu and others},
	journal={arXiv preprint arXiv:2410.13639},
	year={2024}
}
@article{restmcts,
	title={ReST-MCTS*: LLM Self-Training via Process Reward Guided Tree Search},
	author={Zhang, Dan and Zhoubian, Sining and Hu, Ziniu and Yue, Yisong and Dong, Yuxiao and Tang, Jie},
	journal={arXiv preprint arXiv:2406.03816},
	year={2024}
}
@article{googlepvm,
	title={Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning},
	author={Setlur, Amrith and Nagpal, Chirag and Fisch, Adam and Geng, Xinyang and Eisenstein, Jacob and Agarwal, Rishabh and Agarwal, Alekh and Berant, Jonathan and Kumar, Aviral},
	journal={arXiv preprint arXiv:2410.08146},
	year={2024}
}

@article{alphallm,
	title={Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing},
	author={Tian, Ye and Peng, Baolin and Song, Linfeng and Jin, Lifeng and Yu, Dian and Mi, Haitao and Yu, Dong},
	journal={arXiv preprint arXiv:2404.12253},
	year={2024}
}

@article{xie2024monte,
	title={Monte Carlo Tree Search Boosts Reasoning via Iterative Preference Learning},
	author={Xie, Yuxi and Goyal, Anirudh and Zheng, Wenyue and Kan, Min-Yen and Lillicrap, Timothy P and Kawaguchi, Kenji and Shieh, Michael},
	journal={arXiv preprint arXiv:2405.00451},
	year={2024}
}

@article{lai2024step,
	title={Step-dpo: Step-wise preference optimization for long-chain reasoning of llms},
	author={Lai, Xin and Tian, Zhuotao and Chen, Yukang and Yang, Senqiao and Peng, Xiangru and Jia, Jiaya},
	journal={arXiv preprint arXiv:2406.18629},
	year={2024}
}

@article{qwen2.5,
	title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
	author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
	journal={arXiv preprint arXiv:2409.12122},
	year={2024}
}

@article{llama3.1,
	title={The llama 3 herd of models},
	author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
	journal={arXiv preprint arXiv:2407.21783},
	year={2024}
}

@article{rejectionsampling,
	title={Scaling relationship on learning mathematical reasoning with large language models},
	author={Yuan, Zheng and Yuan, Hongyi and Li, Chengpeng and Dong, Guanting and Lu, Keming and Tan, Chuanqi and Zhou, Chang and Zhou, Jingren},
	journal={arXiv preprint arXiv:2308.01825},
	year={2023}
}

@article{wu2024progress,
	title={Progress or regress? self-improvement reversal in post-training},
	author={Wu, Ting and Li, Xuefeng and Liu, Pengfei},
	journal={arXiv preprint arXiv:2407.05013},
	year={2024}
}

@article{deepseekmath,
	title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
	author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
	journal={arXiv preprint arXiv:2402.03300},
	year={2024}
}

@article{yu2023outcome,
	title={Outcome-supervised verifiers for planning in mathematical reasoning},
	author={Yu, Fei and Gao, Anningzhe and Wang, Benyou},
	journal={arXiv preprint arXiv:2311.09724},
	year={2023}
}

@article{o1journeypart2,
	author = {Zhen Huang and Haoyang Zou and Xuefeng Li and Yixiu Liu and Yuxiang Zheng and Ethan Chern and Shijie Xia and Yiwei Qin and Weizhe Yuan and Pengfei Liu},
	title = {O1 Replication Journey – Part 2: Surpassing O1-preview through Simple Distillation Big Progress or Bitter Lesson?},
	year = {2024},
	journal = {Github},
	url = {https://github.com/GAIR-NLP/O1-Journey}
}

@misc{phi3-4k,
	title={Phi-3-mini-4k-instruct}, 
	author={Microsoft},
	year={2024},
	url={https://huggingface.co/microsoft/Phi-3-mini-4k-instruct}
}
@misc{qwen2.5-math-1.5b,
	title={Qwen2.5-Math-1.5B}, 
	author={Qwen},
	year={2024},
	url={https://huggingface.co/Qwen/Qwen2.5-Math-1.5B}
}

@misc{qwen2-math-7b,
	title={Qwen2-Math-7B}, 
	author={Qwen},
year={2024},
url={https://huggingface.co/Qwen/Qwen2-Math-7B}
}


@misc{qwen2.5-math-7b,
	title={Qwen2.5-Math-7B}, 
	author={Qwen},
year={2024},
url={https://huggingface.co/Qwen/Qwen2.5-Math-7B}
}

@misc{qwq-32b-preview,
	title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
	url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
	author = {Qwen Team},
	month = {November},
	year = {2024}
}
@misc{mathstral,
	title = {Mathstral-7B-v0.1},
	url = {https://huggingface.co/mistralai/Mathstral-7B-v0.1},
	author = {The Mistral AI Team},
	year = {2024}
}


@misc{aime,
	title={AIME 2024}, 
	author={AI-MO},
	year={2024},
	url={https://huggingface.co/datasets/AI-MO/aimo-validation-aime}
}

@misc{amc,
	title={AMC 2023}, 
	author={AI-MO},
	year={2024},
	url={https://huggingface.co/datasets/AI-MO/aimo-validation-amc}
}


@misc{numinamathcot,
	title={NuminaMath CoT}, 
	author={Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu},
	year={2024},
	url={https://huggingface.co/datasets/AI-MO/NuminaMath-CoT}
}

@misc{numinamathtir,
	title={NuminaMath TIR}, 
	author={Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu},
	year={2024},
	url={https://huggingface.co/datasets/AI-MO/NuminaMath-TIR}
}
@misc{o1backtracing,
	title={OpenAI's Noam Brown, Ilge Akkaya and Hunter Lightman on o1 and Teaching LLMs to Reason Better}, 
	author={Noam Brown, Ilge Akkaya and Hunter Lightman},
	year={2024},
	url={https://www.youtube.com/watch?v=jPluSXJpdrA}
}



@inproceedings{instructgpt,
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	title = {Training language models to follow instructions with human feedback},
	year = {2024},
	booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
	series = {NIPS '22}
}

@article{huang2023large,
	title={Large language models cannot self-correct reasoning yet},
	author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
	journal={arXiv preprint arXiv:2310.01798},
	year={2023}
}

@article{kumar2024training,
	title={Training language models to self-correct via reinforcement learning},
	author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
	journal={arXiv preprint arXiv:2409.12917},
	year={2024}
}

@misc{vietaformula,
	author   = {Weisstein, Eric W.},
	title    = { Vieta's Formulas, From MathWorld---A Wolfram Web Resource},
	url      = {http://mathworld.wolfram.com/Tree.html},
}



@misc{fermattheorem,
	author   = {Weisstein, Eric W.},
	title    = { Fermat's Little Theorem},
	url      = {https://mathworld.wolfram.com/FermatsLittleTheorem.html},
}

@misc{amgm,
	author   = {},
	title    = {Inequality of Arithmetic and Geometric Means},
	url      = {https://artofproblemsolving.com/wiki/index.php/AM-GM_Inequality},
}

@misc{pythagorean-theorem,
	author   = {},
	title    = {Pythagorean theorem},
	url      = {https://en.wikipedia.org/wiki/Pythagorean_theorem
	},
}

@misc{shoelace-theorem,
	author   = {},
	title    = {Shoelace theorem},
	url      = {https://artofproblemsolving.com/wiki/index.php/Shoelace_Theorem
	},
}

@article{renze2024self,
	title={Self-Reflection in LLM Agents: Effects on Problem-Solving Performance},
	author={Renze, Matthew and Guven, Erhan},
	journal={arXiv preprint arXiv:2405.06682},
	year={2024}
}

@article{shinn2024reflexion,
	title={Reflexion: Language agents with verbal reinforcement learning},
	author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	journal={Advances in Neural Information Processing Systems},
	volume={36},
	year={2024}
}
@article{xin2024deepseek,
	title={DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
	author={Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan},
	journal={arXiv preprint arXiv:2405.14333},
	year={2024}
}
@article{lanham2023measuring,
	title={Measuring faithfulness in chain-of-thought reasoning},
	author={Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and others},
	journal={arXiv preprint arXiv:2307.13702},
	year={2023}
}

@article{deepseekv3,
	title={DeepSeek-V3 Technical Report},
	author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
	journal={arXiv preprint arXiv:2412.19437},
	year={2024}
}
