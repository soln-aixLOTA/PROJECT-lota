\section{Methodology}





\subsection{Design Choices}
\noindent\textbf{MCTS for Effective System 2 Reasoning}. 
We aim to train a math policy SLM and a process reward model (PRM), and integrating both within Monte Carlo Tree Search (MCTS) for System 2 deep thinking. MCTS is chosen for two key reasons. First, it breaks down complex math problems into simpler single-step generation tasks, reducing the difficulty for the policy SLM compared to other System 2 methods like Best-of-N~\citep{brown2024large} or self-consistency~\citep{selfconsistency}, which require generating full solutions in one inference. 
 Second, the step-by-step generation in MCTS naturally yields step-level training data for both models. Standard MCTS rollout  automatically assign Q-value to each step based on its contribution to the final correct answer, obviating the need for human-generated step-level  annotations for process reward model training.

 
 
 
Ideally, advanced LLMs such as GPT-4 could be integrated within MCTS to generate training data. However, this approach faces two key challenges. First,  even these powerful models struggle to consistently solve difficult problems, such as Olympiad-level mathematics. Consequently, the resulting training data would primarily consist of simpler solvable problems, limiting its diversity and quality.  Second, annotating per-step Q-values demands extensive MCTS rollouts; insufficient tree exploration can lead to spurious Q-value assignments, such as overestimating suboptimal steps. Given that each rollout involves multiple single-step generations and these models are computationally expensive, increasing rollouts significantly raises inference costs.
 



\noindent\textbf{Overview}. To this end, we explore using two 7B SLMs (a policy SLM and a PRM) to generate higher-quality training data, with their smaller size allowing for extensive MCTS rollouts on accessible hardware (e.g., 4$\times$40GB A100 GPUs). However, self-generating data presents greater challenges for SLMs, due to their weaker capabilities.
SLMs frequently fail to generate correct solutions, and even when the final answer is correct, the intermediate steps are often flawed or of poor quality. Moreover, SLMs solve fewer challenging problems compared to advanced models like GPT-4.

This section introduces our methodology, as illustrated in Fig.~\ref{fig:method}. To mitigate errors and low-quality intermediate steps, we introduce a code-augmented CoT synthetic method, which performs extensive MCTS rollouts to generate step-by-step verified reasoning trajectories, annotated with   Q-values. To further improve SLM performance on challenging problems, we introduce a four-round self-evolution recipe. In each round, both the policy SLM and the reward model are updated to stronger versions, progressively tackling more difficult problems and generating higher-quality training data. Finally, we present a novel process reward model training approach that eliminates the need for precise per-step reward annotations, yielding the more  
effective process preference model (PPM).






\subsection{Step-by-Step Verified Reasoning Trajectory}
\label{sec:slow thinking}
We start by introducing our method for generating step-by-step verified  reasoning trajectories with per-step Q-value annotations. Given a problem $x$ and a policy model $M$, we run the standard MCTS to incrementally construct a search tree for step-by-step solution exploration. As shown in Fig.~\ref{fig:method}(a), 
the root node represents question $x$, while child nodes correspond to intermediate steps $s$ generated by $M$. A root-to-leaf path ending at  terminal node $s_d$ forms a trajectory $\mathbf{t}=x\oplus s_1\oplus s_2\oplus ...\oplus s_d$, with each step $s_i$  assigned a  Q-value $Q (s_{i})$. 
 From the search tree $\mathcal{T}$, we  extract  solution trajectories $\mathbb{T}=\{\mathbf{t}^1, \mathbf{t}^2, ... , \mathbf{t}^n \} (n\ge1)$. Our goal is to select high-quality trajectories from $\mathcal{T}$ to construct the training set. For this purpose, we introduce code-augmented CoT synthesis method to filter out low-quality generations  and perform extensive rollouts to improve the reliability of Q-value accuracy.




\noindent\textbf{Code-augmented CoT Generation}. Prior MCTS approaches primarily  generate natural language (NL)  CoTs~\citep{rstar,restmcts}. However, LLMs often suffer from hallucination, producing incorrect or irrelevant  steps yet still arrive at the correct answer by chance~\citep{lanham2023measuring}. These flawed steps are challenging to detect and eliminate. To address this, we propose a novel code execution augmented CoT. As shown in Fig.~\ref{fig:promptexample},  the policy model generates a one-step NL CoT alongside its corresponding Python code, where the NL CoT is embedded as a Python comment.  Only generations with successfully executed Python code  are retained as valid candidates. 
\begin{figure*}[ht]
	\centering
	\includegraphics[width=1\textwidth]{promptexample.pdf}	
	\vspace{-4ex}
	\caption{An example of Code-augmented CoT.}	\vspace{-2ex}
	\label{fig:promptexample}
\end{figure*}

Specifically,  starting from the initial root node $x$, we perform multiple MCTS iterations through \textit{selection}, \textit{expansion},  \textit{rollout}, and \textit{back-propagation}. At step $i$, we collect the latest reasoning trajectory $x\oplus s_1\oplus s_2\oplus ...\oplus s_{i-1}$ as the current state. Based on this state, we prompt (see Appendix~\ref{sec:prompt}) the policy model to generate $n$ candidates $s_{i,0}, ..., s_{i, n-1}$ for step $i$. Python code execution is then employed to filter valid nodes. As shown in Fig.~\ref{fig:promptexample}, each generation $s_{i,j}$ is  concatenated with the code from all previous steps, forming $s_1\oplus s_2\oplus ...\oplus s_{i-1}\oplus s_{i,j}$. Candidates that execute successfully are retained as valid nodes and scored by the PPM, which assigns a Q-value $q(s_{i})$. 
Then, we use the well-known Upper Confidence bounds for Trees (UCT)~\citep{utc} to select the best node among the $n$ candidates. This selection process is mathematically represented as: 
\begin{gather}
	\label{eq:uct}
	\text{UCT}(s) =Q(s)+ c \sqrt{\frac{\ln N_{parent}(s)}{N(s)}}; \quad \text{where} \quad Q(s)=\frac{q (s)}{N(s)} 
\end{gather}
where $N(s)$ denotes the number of visits to node $s$, and $N_{\text{parent}}(s)$ is the visit count of $s$'s parent node. The predicted reward $q(s)$ is provided by the PPM and will be updated through back-propagation.  $c$ is a constant that balances exploitation and exploration. 



\noindent\textbf{Extensive Rollouts for Q-value Annotation}. Accurate Q-value $Q(s)$ annotation in Eq.~\ref{eq:uct} is crucial for guiding MCTS node selection towards correct problem-solving paths and identifying high-quality steps within trajectories. 
To improve Q-value reliability, we draw inspiration from Go players, who  retrospectively evaluate the reward of each move based on game outcomes. Although initial estimates may be imprecise, repeated gameplay refines these evaluations over time. Similarly, in each rollout, we update the Q-value of each step based on its contribution to achieving the correct final answer. After extensive MCTS rollouts, steps consistently leading to correct answers achieve higher Q-values, occasional successes yield moderate Q-values, and consistently incorrect steps receive low Q-values. Specifically,  we introduce two self-annotation methods to obtain these step-level Q-values.  Fig.~\ref{fig:method}(c) shows the detailed setting in the four rounds of self-evolution.  

\noindent\textit{Terminal-guided annotation}. During the first two rounds, when the PPM is unavailable or insufficiently accurate, we use  terminal-guided annotation.   Formally,    let $q(s_i)^k$ denote the q value for step $s_i$ after back-propagation in the $k^{th}$ rollout. Following AlphaGo~\citep{alphago} and rStar~\citep{rstar}, we  score each intermediate node based on its contribution to the final correct answer: 
\begin{gather}
	\label{eq:rollout}
	q(s_i)^k = q(s_i)^{k-1}+q(s_d)^{k}; 
\end{gather}
where the initial q value   $q(s_i)^0=0$  in the first rollout. If this step frequently leads to a correct answer,  its $q$ value will increase; otherwise, it decreases. Terminal nodes  are scored as $q(s_d)=1$  for correct answers and $q(s_d)=-1$  otherwise, as shown in Fig.~\ref{fig:method}.


\noindent\textit{PRM-augmented annotation}. Starting from the third round, we use PPM to score each step for more effective generation. Compared to terminal-guided annotation, which requires multiple rollouts for a  meaningful $q$ value,  PPM directly predicts a non-zero initial $q$ value.
PPM-augmented MCTS also helps the policy model to generate higher-quality steps, guiding solutions towards correct paths. Formally, for step $s_i$, PPM predicts an initial $q(s_i)^0$ value  based on the partial trajectory:
\begin{gather}
	\label{eq:qvalue}
	q (s_i)^0=PPM(x\oplus s_1\oplus s_2\oplus ...\oplus s_{i-1}\oplus s_i)    
\end{gather}
This $q$ value will be updated based on terminal node's $q(s_d)$ value through MCTS \textit{back-propagation} in Eq.~\ref{eq:rollout}.
 For terminal node $s_d$, we do not use  PRM for scoring during training data generation. Instead, we assign a more accurate score based on ground truth labels as terminal-guided rewarding.  


\subsection{Process Preference Model}
Process reward models, which provide granular step-level reward signals, is highly desirable for solving challenging math problems. However, obtaining high-quality step-level training data remains an open challenge. Existing methods rely on  human annotations~\citep{lightman2023let} or MCTS-generated scores~\citep{restmcts,alphamath} to assign a score for each step. These scores then serve as training targets, with methods such as MSE loss~\citep{alphamath} or pointwise loss~\citep{mathshepherd,luo2024improve,restmcts} used to minimize the difference between predicted and labeled scores. 
As a result, the precision of these annotated step-level reward scores directly determines the effectiveness of the resulting process reward model. 


Unfortunately, precise per-step scoring remains a unsolved challenge. Although our extensive MCTS rollouts improve the reliability of Q-values, precisely evaluating fine-grained step quality presents a major obstacle. For instance, among a set of correct steps, it is difficult to rank them as best, second-best, or average and then assign precise scores. Similarly, among incorrect steps, differentiating the worst from moderately poor steps poses analogous challenges. Even expert human annotation struggles with consistency, particularly at scale, leading to inherent noise in training labels. 
 
We introduce a novel training method that trains a process preference model (PPM) by constructing step-level positive-negative preference pairs.  As shown in Fig.~\ref{fig:method}(b), instead of using Q-values as direct reward labels,  we use them to select steps from MCTS tree for preference pair construction.  For each step, we select two candidates with the highest Q-values as positive steps and two with the lowest as negative steps. Critically, the selected positive steps must lead to a correct final answer, while negative steps must lead to incorrect answers. For intermediate steps (except the final answer step), the  positive and negative pairs  share the same preceding steps. For the final answer step, where identical reasoning trajectories rarely yield different final answers, we relax this restriction. 
 We select two correct trajectories with the highest average  Q-values as positive examples and two incorrect trajectories with the lowest average Q-values as negative examples. Following~\citep{instructgpt}, we define our loss function using the standard Bradley-Terry model with a pairwise ranking loss: 
\vspace{-1ex}
 \begin{gather}
 	\label{eq:ppm}
 	\mathcal{L}_{ppm}(\theta)=-\frac{1}{2\times2}E_{(x,y_{i}^{pos}, y_{i}^{neg}\in \mathbb{D})}[log(\sigma(r_{\theta}(x,y_{i}^{pos})-r_{\theta}(x,y_{i}^{neg})))] \\
 	 \text{when $i$ is not final answer step},  y_{i}^{pos}=s_1\oplus...\oplus s_{i-1}\oplus s_{i}^{pos}; y_{i}^{neg}=s_1\oplus...\oplus s_{i-1}\oplus s_{i}^{neg}
 	   \vspace{-1ex}
 \end{gather}
Here, $r_{\theta}(x,y_i)$ denotes the output of the PPM, where $x$ is the problem and $y$  is the trajectory from the first step to the $i^{th}$ step.





\subsection{Self-Evolved Deep Thinking} 
\label{sec:selfevolution}
\subsubsection{Training with Step-by-Step Verified Reasoning Trajectory}
\vspace{-1ex}
\noindent\textbf{Math Problems Collection}. We collect a large dataset of 747k math word problems with final answer ground-truth labels, primarily from  NuminaMath~\citep{numina_math_datasets} and MetaMath~\citep{yu2023metamath}. Notably, only competition-level problems (e.g., Olympiads and AIME/AMC) from NuminaMath are included, as we  observe that  grade-school-level problems do not significantly improve LLM complex math reasoning. To augment the limited  competition-level problems, we follow ~\citep{xwin} and use GPT-4 to synthesize new problems based on the seed problems in 7.5k MATH train set and 3.6k AMC-AIME training split.  However, GPT-4 often generated unsolvable problems or incorrect solutions for challenging seed problems. To filter these, we prompt GPT-4 to generate 10 solutions per problem, retaining only those with at least 3 consistent solutions. %, which are then accepted as the ground truth.

\noindent\textbf{Reasoning Trajectories Collection}. Instead of using the original solutions in the 747k math dataset, we conduct extensive MCTS rollouts (Sec.~\ref{sec:slow thinking}) to generate higher-quality step-by-step verified reasoning trajectories.  In each self-evolution round,   we perform 16 rollouts per math problem, which leads to 16 reasoning trajectories. Problems are then categories by difficulty based on the correct ratio of the generated trajectories:  \textit{easy} (all solutions are correct), \textit{medium} (a mix of correct and incorrect solutions) and \textit{hard} (all solutions are incorrect). For \textit{hard}  problems with no correct trajectories, an additional MCTS with 16 rollouts is performed.  After that, all step-by-step trajectories and their annotated Q-values are collected and filtered to train the policy SLM and process preference model.

\noindent\textbf{Supervised Fine-tuning the Policy SLM}.  Through extensive experiments, we find that selecting high-quality reasoning trajectories is the key for fine-tuning a frontier math LLM. While  methods such as  GPT-distillation and Best-of-N can include low-quality or erroneous intermediate steps, a more effective approach ensures that every step in the trajectory is of high quality. To achieve this, we use per-step Q-values to select optimal trajectories from MCTS rollouts. Specifically, for each math problem, we select the top-2 trajectories with the highest average Q-values among those leading to  correct answers as SFT training data.



\noindent\textbf{Training PPM}. The PPM is initialized from the fine-tuned policy model, with its next-token prediction head replaced by a scalar-value head consisting of  a linear layer and a tanh function to constrain outputs to the range [-1, 1].  We filter out math problems where all solution trajectories are fully correct or incorrect.   For problems with mixed outcomes, we select two positive and two negative examples for each step based on Q-values, which are used as preference pairs for  training data.




\subsubsection{Recipe for Self-Evolution}

\vspace{-1ex}
\begin{table*}[hpt]
	\small 
	\centering
	\caption{Percentage of the 747k math problems  correctly solved in each round. Only problems have correct solutions are included in the  training set.   The first round uses DeepSeek-Coder-Instruct as the policy LLM, while later rounds use our fine-tuned 7B policy SLM.}
	\label{tbl:solveratio}
		\begin{tabular}
			{lccccc}
			\toprule
			\#&models in MCTS&GSM-level&MATH-level&Olympiad-level &All\\
			\midrule 
			Round 1&DeepSeek-Coder-V2-Instruct &96.61\%&67.36\% & 20.99\% & 60.17\%\\
			Round 2 &policy SLM-r1&97.88\%& 67.40\%& 56.04\% &66.60\%\\
			Round 3&policy SLM-r2, PPM-r2 &98.15\%& 88.69\%& 62.16\%&77.86\%\\
			Round 4&policy SLM-r3, PPM-r3 &98.15\%&94.53\% &80.58\% &90.25\%\\
			\hline
	\end{tabular}%}
		\vspace{-1ex}
\end{table*}

\begin{table*}[hpt]
	\small 
	\centering
	\caption{Pass@1 accuracy of the resulting policy SLM in each round, showing continuous improvement until surpassing the bootstrap model.}
	\label{tbl:policyllm}
		\resizebox{1\textwidth}{!}{
		\begin{tabular}%{@{\hskip0pt}c@{\hskip4pt}|cccc@{\hskip0pt}}
			{@{\hskip0pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip6pt}c@{\hskip0pt}}
			\toprule
		Round\#	&MATH&AIME 2024& AMC 2023 & Olympiad Bench& College Math &GSM8K&GaokaoEn 2023  \\
			\midrule 
	\makecell{DeepSeek-Coder-V2-Instruct\\(bootstrap model)} & 75.3&13.3 & 57.5	& 37.6&46.2& 94.9& 64.7 \\
			\midrule
				Base (Qwen2.5-Math-7B) & 58.8&0.0&22.5&21.8&41.6&91.6&51.7\\
				\hdashline
	policy	SLM-r1 & 69.6&3.3& 30.0& 34.7& 44.5& 88.4& 57.4\\
	policy	SLM-r2 & 73.6& 10.0& 35.0& 39.0& 45.7& 89.1& 59.7\\
	policy	SLM-r3 & 75.8& 16.7& 45.0& 44.1& 49.6& 89.3& 62.8 \\
	policy	SLM-r4& 78.4& 26.7 & 47.5&47.1 &  52.5&89.7 & 65.7\\
			\hline
		\end{tabular}}
		\vspace{-1ex}
\end{table*}

\begin{table*}[hpt]
	\small 
	\centering
	\caption{The quality of PPM consistently improves across rounds. The policy model has been fixed with policy SLM-r1 for a fair comparison.}
	\label{tbl:prm-evolution}
	\resizebox{1\textwidth}{!}{
		\begin{tabular}%{@{\hskip0pt}c@{\hskip4pt}|cccc@{\hskip0pt}}
			{lccccccc}
			\toprule
			Round\#&MATH&AIME 2024& AMC 2023& Olympiad Bench& College Math &GSM8K&GaokaoEn 2023  \\
			\midrule 
			PPM-r1  &75.2 &10.0 &57.5&35.7&45.4& 90.9&60.3  \\
			PPM-r2 & 84.1& 26.7&75.0&52.7&54.2& 93.3&73.0    \\
			PPM-r3  &85.2&33.3 &77.5&59.5 &  55.6&93.9&76.6  \\
			PPM-r4  &87.0 &43.3& 77.5&61.5  & 56.8& 94.2&77.8  \\
			\hline
	\end{tabular}}
	%	\vspace{-1ex}
\end{table*}
Due to the weaker capabilities of SLMs, we perform four rounds of  MCTS deep thinking to progressively generate higher-quality data and expand the training set with more challenging math problems. Each round uses MCTS to generate step-by-step verified reasoning trajectories, which are then used to train the new policy SLM and PPM. The new models are then applied in  next round to  generate higher-quality training data. Fig.~\ref{fig:method}(c) and Table~\ref{tbl:solveratio}  detail the models used for data generation in each round, along with the identifiers of the trained policy model and PPM. Next, we outline the details and specific improvements targeted in each round.




\noindent\textbf{Round 1: Bootstrapping an initial strong policy SLM-r1}. To enable SLMs to self-generate reasonably good training data, we perform a bootstrap round to fine-tune an initial strong policy model, denoted as SLM-r1. 
As shown in Table~\ref{tbl:solveratio}, we run MCTS with DeepSeek-Coder-V2-Instruct (236B)  to collect the SFT data. With no available reward model in this round, we use terminal-guided annotation for Q-values and limit MCTS to 8 rollouts for efficiency. For correct solutions, the top-2 trajectories with the highest average Q-values are selected as SFT data.   We also train PPM-r1, but the limited rollouts yields unreliable Q-values, affecting the effectiveness of PPM-r1 ( Table~\ref{tbl:prm-evolution}). 



\noindent\textbf{Round 2: Training a reliable PPM-r2}. In this round, with the policy model updated to the 7B SLM-r1, we conduct extensive MCTS rollouts for more reliable Q-value annotation and train the first reliable reward model, PPM-r2. Specifically, we perform 16 MCTS rollouts per problem. The resulting step-by-step verified reasoning trajectories show significant improvements in both quality and Q-value precision. As shown in Table~\ref{tbl:prm-evolution}, PPM-r2 is notably more effective than in the bootstrap round. Moreover,  the policy SLM-r2 also continues to improve as expected (Table~\ref{tbl:policyllm}).


 

\noindent\textbf{Round 3: PPM-augmented MCTS to significantly improve data quality}. With the reliable PPM-r2, we perform PPM-augmented MCTS in this round to generate data, leading to significantly higher-quality trajectories that cover more math and Olympiad-level problems in the training set (Table~\ref{tbl:solveratio}). The generated reasoning trajectories and self-annotated Q-values are then used to train the new policy SLM-r3 and PPM-r3, both of which show significant improvements. 

\noindent\textbf{Round 4: Solving challenging math problems}. After the third round, while grade school and MATH problems achieve high success rates, only 62.16\% of Olympiad-level problems are included in the training set. This is \emph{NOT} solely due to weak reasoning abilities in our SLMs, as many Olympiad problems remain unsolved by GPT-4 or o1. To improve coverage, we adopt a straightforward strategy. For unsolved problems after 16 MCTS rollouts, we perform an additional 64 rollouts, and if needed, increase to 128. We also conduct multiple MCTS tree expansions with different random seeds.  This  boosts the success rate of Olympiad-level problems to 80.58\%.





After four rounds of self-evolution, 90.25\% of the 747k math problems are successfully covered into the training set, as shown in Table ~\ref{tbl:solveratio}. Among the remaining unsolved problems, a significant portion consists of synthetic questions. We manually review a random sample of 20 problems and find that 19 are incorrectly labeled with  wrong answers. Based on this, we conclude that the remaining unsolved problems are of low quality and thus terminate the self-evolution at round 4. 

















