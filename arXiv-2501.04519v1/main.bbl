\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[amg()]{amgm}
Inequality of arithmetic and geometric means.
\newblock URL
  \url{https://artofproblemsolving.com/wiki/index.php/AM-GM_Inequality}.

\bibitem[pyt()]{pythagorean-theorem}
Pythagorean theorem.
\newblock URL \url{https://en.wikipedia.org/wiki/Pythagorean_theorem}.

\bibitem[sho()]{shoelace-theorem}
Shoelace theorem.
\newblock URL
  \url{https://artofproblemsolving.com/wiki/index.php/Shoelace_Theorem}.

\bibitem[Abdin et~al.(2024)Abdin, Jacobs, Awan, Aneja, Awadallah, Awadalla,
  Bach, Bahree, Bakhtiari, Behl, et~al.]{phi3}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,
  Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl,
  et~al.
\newblock Phi-3 technical report: A highly capable language model locally on
  your phone.
\newblock \emph{arXiv preprint arXiv:2404.14219}, 2024.

\bibitem[AI-MO(2024{\natexlab{a}})]{aime}
AI-MO.
\newblock Aime 2024, 2024{\natexlab{a}}.
\newblock URL \url{https://huggingface.co/datasets/AI-MO/aimo-validation-aime}.

\bibitem[AI-MO(2024{\natexlab{b}})]{amc}
AI-MO.
\newblock Amc 2023, 2024{\natexlab{b}}.
\newblock URL \url{https://huggingface.co/datasets/AI-MO/aimo-validation-amc}.

\bibitem[Brown et~al.(2024)Brown, Juravsky, Ehrlich, Clark, Le, R{\'e}, and
  Mirhoseini]{brown2024large}
Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc~V Le,
  Christopher R{\'e}, and Azalia Mirhoseini.
\newblock Large language monkeys: Scaling inference compute with repeated
  sampling.
\newblock \emph{arXiv preprint arXiv:2407.21787}, 2024.

\bibitem[Chen et~al.(2024)Chen, Liao, Li, and Fan]{alphamath}
Guoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan.
\newblock Alphamath almost zero: process supervision without process, 2024.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{gsm8k}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz
  Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
  et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Daniel(2011)]{thinking}
Kahneman Daniel.
\newblock Thinking, fast and slow.
\newblock \emph{Macmillan}, 2011.

\bibitem[Dubey et~al.(2024)Dubey, Jauhri, Pandey, Kadian, Al-Dahle, Letman,
  Mathur, Schelten, Yang, Fan, et~al.]{llama3.1}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
  Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
  et~al.
\newblock The llama 3 herd of models.
\newblock \emph{arXiv preprint arXiv:2407.21783}, 2024.

\bibitem[Gou et~al.(2023)Gou, Shao, Gong, Yang, Huang, Duan, Chen,
  et~al.]{tora}
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan,
  Weizhu Chen, et~al.
\newblock Tora: A tool-integrated reasoning agent for mathematical problem
  solving.
\newblock \emph{arXiv preprint arXiv:2309.17452}, 2023.

\bibitem[Hao et~al.(2023)Hao, Gu, Ma, Hong, Wang, Wang, and Hu]{rap}
Shibo Hao, Yi~Gu, Haodi Ma, Joshua~Jiahua Hong, Zhen Wang, Daisy~Zhe Wang, and
  Zhiting Hu.
\newblock Reasoning with language model is planning with world model.
\newblock \emph{arXiv preprint arXiv:2305.14992}, 2023.

\bibitem[He et~al.(2024)He, Luo, Bai, Hu, Thai, Shen, Hu, Han, Huang, Zhang,
  et~al.]{he2024olympiadbench}
Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen~Leng Thai, Junhao Shen,
  Jinyi Hu, Xu~Han, Yujie Huang, Yuxiang Zhang, et~al.
\newblock Olympiadbench: A challenging benchmark for promoting agi with
  olympiad-level bilingual multimodal scientific problems.
\newblock \emph{arXiv preprint arXiv:2402.14008}, 2024.

\bibitem[Huang et~al.(2023)Huang, Chen, Mishra, Zheng, Yu, Song, and
  Zhou]{huang2023large}
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu~Steven Zheng, Adams~Wei Yu,
  Xinying Song, and Denny Zhou.
\newblock Large language models cannot self-correct reasoning yet.
\newblock \emph{arXiv preprint arXiv:2310.01798}, 2023.

\bibitem[Huang et~al.(2024)Huang, Zou, Li, Liu, Zheng, Chern, Xia, Qin, Yuan,
  and Liu]{o1journeypart2}
Zhen Huang, Haoyang Zou, Xuefeng Li, Yixiu Liu, Yuxiang Zheng, Ethan Chern,
  Shijie Xia, Yiwei Qin, Weizhe Yuan, and Pengfei Liu.
\newblock O1 replication journey – part 2: Surpassing o1-preview through
  simple distillation big progress or bitter lesson?
\newblock \emph{Github}, 2024.
\newblock URL \url{https://github.com/GAIR-NLP/O1-Journey}.

\bibitem[Jia~LI and Polu(2024{\natexlab{a}})]{numina_math_datasets}
Lewis Tunstall Ben Lipkin Roman Soletskyi Shengyi Costa Huang Kashif Rasul
  Longhui Yu Albert Jiang Ziju Shen Zihan Qin Bin Dong Li Zhou Yann Fleureau
  Guillaume~Lample Jia~LI, Edward~Beeching and Stanislas Polu.
\newblock Numinamath.
\newblock
  \url{[https://github.com/project-numina/aimo-progress-prize](https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf)},
  2024{\natexlab{a}}.

\bibitem[Jia~LI and Polu(2024{\natexlab{b}})]{numinamathcot}
Lewis Tunstall Ben Lipkin Roman Soletskyi Shengyi Costa Huang Kashif Rasul
  Longhui Yu Albert Jiang Ziju Shen Zihan Qin Bin Dong Li Zhou Yann Fleureau
  Guillaume~Lample Jia~LI, Edward~Beeching and Stanislas Polu.
\newblock Numinamath cot, 2024{\natexlab{b}}.
\newblock URL \url{https://huggingface.co/datasets/AI-MO/NuminaMath-CoT}.

\bibitem[Kang et~al.(2024)Kang, Li, Chen, Kazemi, and Chen]{kang2024mindstar}
Jikun Kang, Xin~Zhe Li, Xi~Chen, Amirreza Kazemi, and Boxing Chen.
\newblock Mindstar: Enhancing math reasoning in pre-trained llms at inference
  time.
\newblock \emph{arXiv preprint arXiv:2405.16265}, 2024.

\bibitem[Kocsis and Szepesvári(2006)]{utc}
Levente Kocsis and Csaba Szepesvári.
\newblock Bandit based monte-carlo planning.
\newblock volume 2006, pages 282--293, 09 2006.
\newblock ISBN 978-3-540-45375-8.
\newblock \doi{10.1007/11871842_29}.

\bibitem[Kumar et~al.(2024)Kumar, Zhuang, Agarwal, Su, Co-Reyes, Singh, Baumli,
  Iqbal, Bishop, Roelofs, et~al.]{kumar2024training}
Aviral Kumar, Vincent Zhuang, Rishabh Agarwal, Yi~Su, John~D Co-Reyes, Avi
  Singh, Kate Baumli, Shariq Iqbal, Colton Bishop, Rebecca Roelofs, et~al.
\newblock Training language models to self-correct via reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2409.12917}, 2024.

\bibitem[Lanham et~al.(2023)Lanham, Chen, Radhakrishnan, Steiner, Denison,
  Hernandez, Li, Durmus, Hubinger, Kernion, et~al.]{lanham2023measuring}
Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison,
  Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion,
  et~al.
\newblock Measuring faithfulness in chain-of-thought reasoning.
\newblock \emph{arXiv preprint arXiv:2307.13702}, 2023.

\bibitem[Li et~al.(2024)Li, Wang, Hu, Wei, Zheng, Hu, Zhang, and Peng]{xwin}
Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu, Zheng
  Zhang, and Houwen Peng.
\newblock Common 7b language models already possess strong math capabilities.
\newblock \emph{arXiv preprint arXiv:2403.04706}, 2024.

\bibitem[Liao et~al.(2024)Liao, Luo, Li, Wu, and Fan]{liao2024mario}
Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, and Kai Fan.
\newblock Mario: Math reasoning with code interpreter output--a reproducible
  pipeline.
\newblock \emph{arXiv preprint arXiv:2401.08190}, 2024.

\bibitem[Lightman et~al.(2023)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2023let}
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy
  Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock \emph{arXiv preprint arXiv:2305.20050}, 2023.

\bibitem[Lightman et~al.(2024)Lightman, Kosaraju, Burda, Edwards, Baker, Lee,
  Leike, Schulman, Sutskever, and Cobbe]{lightman2024lets}
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker,
  Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
\newblock Let's verify step by step.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=v8L0pN6EOi}.

\bibitem[Liu et~al.(2024)Liu, Feng, Xue, Wang, Wu, Lu, Zhao, Deng, Zhang, Ruan,
  et~al.]{deepseekv3}
Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang
  Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et~al.
\newblock Deepseek-v3 technical report.
\newblock \emph{arXiv preprint arXiv:2412.19437}, 2024.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and
  Zhang]{luo2023wizardmath}
Haipeng Luo, Qingfeng Sun, Can Xu, Pu~Zhao, Jianguang Lou, Chongyang Tao, Xiubo
  Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang.
\newblock Wizardmath: Empowering mathematical reasoning for large language
  models via reinforced evol-instruct.
\newblock \emph{arXiv preprint arXiv:2308.09583}, 2023.

\bibitem[Luo et~al.(2024)Luo, Liu, Liu, Phatale, Lara, Li, Shu, Zhu, Meng, Sun,
  et~al.]{luo2024improve}
Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Harsh Lara, Yunxuan
  Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, et~al.
\newblock Improve mathematical reasoning in language models by automated
  process supervision.
\newblock \emph{arXiv preprint arXiv:2406.06592}, 2024.

\bibitem[Microsoft(2024)]{phi3-4k}
Microsoft.
\newblock Phi-3-mini-4k-instruct, 2024.
\newblock URL \url{https://huggingface.co/microsoft/Phi-3-mini-4k-instruct}.

\bibitem[Noam~Brown and Lightman(2024)]{o1backtracing}
Ilge~Akkaya Noam~Brown and Hunter Lightman.
\newblock Openai's noam brown, ilge akkaya and hunter lightman on o1 and
  teaching llms to reason better, 2024.
\newblock URL \url{https://www.youtube.com/watch?v=jPluSXJpdrA}.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock 2023.

\bibitem[OpenAI(2024)]{o1}
OpenAI.
\newblock Openai o1 system card.
\newblock \emph{preprint}, 2024.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 27730--27744, 2022.

\bibitem[Qi et~al.(2024)Qi, Ma, Xu, Zhang, Yang, and Yang]{rstar}
Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li~Lyna Zhang, Fan Yang, and Mao Yang.
\newblock Mutual reasoning makes smaller llms stronger problem-solvers.
\newblock \emph{arXiv preprint arXiv:2408.06195}, 2024.

\bibitem[Qwen(2024{\natexlab{a}})]{qwen2-math-7b}
Qwen.
\newblock Qwen2-math-7b, 2024{\natexlab{a}}.
\newblock URL \url{https://huggingface.co/Qwen/Qwen2-Math-7B}.

\bibitem[Qwen(2024{\natexlab{b}})]{qwen2.5-math-1.5b}
Qwen.
\newblock Qwen2.5-math-1.5b, 2024{\natexlab{b}}.
\newblock URL \url{https://huggingface.co/Qwen/Qwen2.5-Math-1.5B}.

\bibitem[Qwen(2024{\natexlab{c}})]{qwen2.5-math-7b}
Qwen.
\newblock Qwen2.5-math-7b, 2024{\natexlab{c}}.
\newblock URL \url{https://huggingface.co/Qwen/Qwen2.5-Math-7B}.

\bibitem[Renze and Guven(2024)]{renze2024self}
Matthew Renze and Erhan Guven.
\newblock Self-reflection in llm agents: Effects on problem-solving
  performance.
\newblock \emph{arXiv preprint arXiv:2405.06682}, 2024.

\bibitem[Shinn et~al.(2024)Shinn, Cassano, Gopinath, Narasimhan, and
  Yao]{shinn2024reflexion}
Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu
  Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{alphago}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Snell et~al.(2024)Snell, Lee, Xu, and Kumar]{snell2024scaling}
Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar.
\newblock Scaling llm test-time compute optimally can be more effective than
  scaling model parameters.
\newblock \emph{arXiv preprint arXiv:2408.03314}, 2024.

\bibitem[Tang et~al.(2024)Tang, Zhang, Wan, and Wei]{mathscale}
Zhengyang Tang, Xingxing Zhang, Benyou Wan, and Furu Wei.
\newblock Mathscale: Scaling instruction tuning for mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2403.02884}, 2024.

\bibitem[Team(2024{\natexlab{a}})]{qwq-32b-preview}
Qwen Team.
\newblock Qwq: Reflect deeply on the boundaries of the unknown, November
  2024{\natexlab{a}}.
\newblock URL \url{https://qwenlm.github.io/blog/qwq-32b-preview/}.

\bibitem[Team(2024{\natexlab{b}})]{mathstral}
The Mistral~AI Team.
\newblock Mathstral-7b-v0.1, 2024{\natexlab{b}}.
\newblock URL \url{https://huggingface.co/mistralai/Mathstral-7B-v0.1}.

\bibitem[Toshniwal et~al.(2024)Toshniwal, Du, Moshkov, Kisacanin, Ayrapetyan,
  and Gitman]{openmath2}
Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan
  Ayrapetyan, and Igor Gitman.
\newblock Openmathinstruct-2: Accelerating ai for math with massive open-source
  instruction data.
\newblock \emph{arXiv preprint arXiv:2410.01560}, 2024.

\bibitem[Valmeekam et~al.(2023)Valmeekam, Sreedharan, Marquez, Olmo, and
  Kambhampati]{valmeekam2023planning}
Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and
  Subbarao Kambhampati.
\newblock On the planning abilities of large language models (a critical
  investigation with a proposed benchmark).
\newblock \emph{arXiv preprint arXiv:2302.06706}, 2023.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Deng, Lv, Yan, and
  Bo]{wang2024qimprovingmultistepreasoning}
Chaojie Wang, Yanchen Deng, Zhiyi Lv, Shuicheng Yan, and An~Bo.
\newblock Q*: Improving multi-step reasoning for llms with deliberative
  planning, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Ren, Zhou, Lu, Luo, Shi, Zhang,
  Song, Zhan, and Li]{wang2024mathcoder}
Ke~Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun Luo, Weikang Shi, Renrui
  Zhang, Linqi Song, Mingjie Zhan, and Hongsheng Li.
\newblock Mathcoder: Seamless code integration in {LLM}s for enhanced
  mathematical reasoning.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=z8TW0ttBPp}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Li, Shao, Xu, Dai, Li, Chen, Wu,
  and Sui]{mathshepherd}
Peiyi Wang, Lei Li, Zhihong Shao, R.~X. Xu, Damai Dai, Yifei Li, Deli Chen,
  Y.~Wu, and Zhifang Sui.
\newblock Math-shepherd: Verify and reinforce llms step-by-step without human
  annotations, 2024{\natexlab{c}}.

\bibitem[Wang et~al.(2023)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H. Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=1PL1NIMMrw}.

\bibitem[Weisstein({\natexlab{a}})]{fermattheorem}
Eric~W. Weisstein.
\newblock Fermat's little theorem, {\natexlab{a}}.
\newblock URL \url{https://mathworld.wolfram.com/FermatsLittleTheorem.html}.

\bibitem[Weisstein({\natexlab{b}})]{vietaformula}
Eric~W. Weisstein.
\newblock Vieta's formulas, from mathworld---a wolfram web resource,
  {\natexlab{b}}.
\newblock URL \url{http://mathworld.wolfram.com/Tree.html}.

\bibitem[Wu et~al.(2024)Wu, Sun, Li, Welleck, and Yang]{wu2024empirical}
Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang.
\newblock An empirical analysis of compute-optimal inference for
  problem-solving with language models.
\newblock \emph{arXiv preprint arXiv:2408.00724}, 2024.

\bibitem[Xin et~al.(2024)Xin, Guo, Shao, Ren, Zhu, Liu, Ruan, Li, and
  Liang]{xin2024deepseek}
Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo~Liu, Chong
  Ruan, Wenda Li, and Xiaodan Liang.
\newblock Deepseek-prover: Advancing theorem proving in llms through
  large-scale synthetic data.
\newblock \emph{arXiv preprint arXiv:2405.14333}, 2024.

\bibitem[Yang et~al.(2024)Yang, Zhang, Hui, Gao, Yu, Li, Liu, Tu, Zhou, Lin,
  et~al.]{qwen2.5}
An~Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li,
  Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et~al.
\newblock Qwen2. 5-math technical report: Toward mathematical expert model via
  self-improvement.
\newblock \emph{arXiv preprint arXiv:2409.12122}, 2024.

\bibitem[Yao et~al.(2024)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and
  Narasimhan]{tot}
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and
  Karthik Narasimhan.
\newblock Tree of thoughts: Deliberate problem solving with large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Yu et~al.(2023{\natexlab{a}})Yu, Gao, and Wang]{yu2023outcome}
Fei Yu, Anningzhe Gao, and Benyou Wang.
\newblock Outcome-supervised verifiers for planning in mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2311.09724}, 2023{\natexlab{a}}.

\bibitem[Yu et~al.(2023{\natexlab{b}})Yu, Jiang, Shi, Yu, Liu, Zhang, Kwok, Li,
  Weller, and Liu]{yu2023metamath}
Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu~Zhang,
  James~T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu.
\newblock Metamath: Bootstrap your own mathematical questions for large
  language models.
\newblock \emph{arXiv preprint arXiv:2309.12284}, 2023{\natexlab{b}}.

\bibitem[Yuan et~al.(2023)Yuan, Yuan, Li, Dong, Lu, Tan, Zhou, and
  Zhou]{rejectionsampling}
Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan,
  Chang Zhou, and Jingren Zhou.
\newblock Scaling relationship on learning mathematical reasoning with large
  language models.
\newblock \emph{arXiv preprint arXiv:2308.01825}, 2023.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Zhoubian, Hu, Yue, Dong, and
  Tang]{restmcts}
Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang.
\newblock Rest-mcts*: Llm self-training via process reward guided tree search.
\newblock \emph{arXiv preprint arXiv:2406.03816}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Li, Huang, Zhou, Li, and
  Ouyang]{zhang2024accessing}
Di~Zhang, Jiatong Li, Xiaoshui Huang, Dongzhan Zhou, Yuqiang Li, and Wanli
  Ouyang.
\newblock Accessing gpt-4 level mathematical olympiad solutions via monte carlo
  tree self-refine with llama-3 8b.
\newblock \emph{arXiv preprint arXiv:2406.07394}, 2024{\natexlab{b}}.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, Zhang, Gonzalez, and Stoica]{llmjudge}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph~E.
  Gonzalez, and Ion Stoica.
\newblock Judging {LLM}-as-a-judge with {MT}-bench and chatbot arena.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2023.

\end{thebibliography}
